{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlay station data to grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = Path(\"../../data/01-raw\")\n",
    "PROCESSED_PATH = Path(\"../../data/02-processed\")\n",
    "RESULTS_PATH = Path(\"../../data/03-results\")\n",
    "RESOLUTION = 0.02  # 2 km\n",
    "CROP_ALLOWANCE_DEG = 13 * RESOLUTION\n",
    "CITY_NAME = \"Dagupan\"\n",
    "YEAR = 2007\n",
    "\n",
    "DOMAINS_GEOJSON = RAW_PATH / \"domains/downscaling_domains_fixed.geojson\"\n",
    "STATION_LOCATION_CSV = RAW_PATH / \"station_data/PAGASA_station_locations.csv\"\n",
    "STATION_DATA_CSV = PROCESSED_PATH / \"station_data.csv\"\n",
    "VARS_NC = (\n",
    "    PROCESSED_PATH\n",
    "    / f\"input/chirts_chirps_regridded_interpolated_{CITY_NAME.lower()}.nc\"\n",
    ")\n",
    "VARS = [\"precip\", \"tmax\", \"tmin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load station location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_locations_df = pd.read_csv(STATION_LOCATION_CSV)\n",
    "station_locations_df.head()\n",
    "station_lat = station_locations_df.loc[\n",
    "    station_locations_df[\"station_name\"] == CITY_NAME, \"lat\"\n",
    "]\n",
    "station_lon = station_locations_df.loc[\n",
    "    station_locations_df[\"station_name\"] == CITY_NAME, \"lon\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pd.read_csv(STATION_DATA_CSV)\n",
    "station_df = (\n",
    "    stations_df[stations_df[\"station\"] == CITY_NAME]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .replace(-999, np.nan)\n",
    "    .rename(columns={\"rainfall\": \"precip\"})\n",
    ")\n",
    "station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds_gdf = gpd.read_file(DOMAINS_GEOJSON, driver=\"GeoJSON\")\n",
    "# station_centroid = (\n",
    "#     bounds_gdf[bounds_gdf[\"city\"] == CITY_NAME]\n",
    "#     .to_crs(3857)\n",
    "#     .centroid\n",
    "#     .to_crs(4326)\n",
    "# )\n",
    "# station_lat = station_centroid.y\n",
    "# station_lon = station_centroid.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ds = xr.Dataset(\n",
    "    data_vars={\n",
    "        var: (\n",
    "            [\"time\", \"lat\", \"lon\"],\n",
    "            station_df[var].to_numpy().reshape((len(station_df[\"date\"]), 1, 1)),\n",
    "        )\n",
    "        for var in VARS\n",
    "    },\n",
    "    coords=dict(\n",
    "        time=(\"time\", pd.DatetimeIndex(station_df[\"date\"])),\n",
    "        lon=(\"lon\", station_lon),\n",
    "        lat=(\"lat\", station_lat),\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"Station data\",\n",
    "    ),\n",
    ")\n",
    "station_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_tmin_da = station_ds[\"tmin\"]\n",
    "station_tmin_da[:, 0, 0].plot(marker=\"o\", linestyle=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_tmax_da = station_ds[\"tmax\"]\n",
    "station_tmax_da[:, 0, 0].plot(marker=\"o\", linestyle=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_precip_da = station_ds[\"precip\"]\n",
    "station_precip_da[:, 0, 0].plot(marker=\"o\", linestyle=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gridded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_ds = xr.open_dataset(VARS_NC, engine=\"scipy\").sel(band=1)\n",
    "gridded_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_ds[\"tmin\"].sel(time=\"2008-07-01\", method=\"nearest\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_tmin_da.sel(time=\"2008-07-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_da = gridded_ds[\"tmin\"].sel(time=\"2008-07-01\", method=\"nearest\")\n",
    "gridded_tmin_da.plot(vmin=20, vmax=31)\n",
    "plt.plot(station_lon, station_lat, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_station_da = gridded_tmin_da.where(\n",
    "    (gridded_tmin_da.lat >= (station_lat.item() - 0.125))\n",
    "    & (gridded_tmin_da.lat <= (station_lat.item() + 0.125))\n",
    "    & (gridded_tmin_da.lon >= (station_lon.item() - 0.125))\n",
    "    & (gridded_tmin_da.lon <= (station_lon.item() + 0.125)),\n",
    "    drop=True,\n",
    ")\n",
    "gridded_tmin_station_da.plot(vmin=20, vmax=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_station_da.plot.hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100)\n",
    "\n",
    "# assume normal distribution with mean centered at station data\n",
    "# and standard deviation one-tenth of the said value\n",
    "station_tmin_mean = station_tmin_da.sel(time=\"2008-07-01\").values[0]\n",
    "station_tmin_std = station_tmin_mean / 10\n",
    "\n",
    "gridded_tmin_station_arr = gridded_tmin_station_da.values\n",
    "gridded_tmin_station_mean = np.nanmean(gridded_tmin_station_arr)\n",
    "gridded_tmin_station_std = np.nanstd(gridded_tmin_station_arr)\n",
    "\n",
    "# x = np.linspace(station_tmin_mean-4*station_tmin_std,station_tmin_mean+4*station_tmin_std,100)\n",
    "x_w_nan = gridded_tmin_station_da.values.flatten()\n",
    "x = x_w_nan[~np.isnan(x_w_nan)]\n",
    "plt.plot(\n",
    "    x, norm.pdf(x, loc=station_tmin_mean, scale=station_tmin_std), \"o\", label=\"station\"\n",
    ")\n",
    "plt.plot(\n",
    "    x,\n",
    "    norm.pdf(x, loc=gridded_tmin_station_mean, scale=gridded_tmin_station_std),\n",
    "    \"o\",\n",
    "    label=\"gridded\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = gridded_tmin_station_da.values.flatten()\n",
    "ratio_df = pd.DataFrame(\n",
    "    dict(\n",
    "        n=n,\n",
    "        ratio=(\n",
    "            norm.pdf(n, loc=station_tmin_mean, scale=station_tmin_std)\n",
    "            / norm.pdf(n, loc=gridded_tmin_station_mean, scale=gridded_tmin_station_std)\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame(\n",
    "    dict(\n",
    "        x=x,\n",
    "        station_dist=norm.pdf(x, loc=station_tmin_mean, scale=station_tmin_std),\n",
    "        gridded_dist=norm.pdf(\n",
    "            x, loc=gridded_tmin_station_mean, scale=gridded_tmin_station_std\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "dist_df[\"dist_ratio\"] = dist_df[\"gridded_dist\"] / dist_df[\"station_dist\"]\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_replace(da, to_replace, value):\n",
    "    \"\"\"\n",
    "    From https://github.com/pydata/xarray/issues/6377#issue-1173497454\n",
    "    \"\"\"\n",
    "    # Use np.unique to create an inverse index\n",
    "    flat = da.values.ravel()\n",
    "    uniques, index = np.unique(flat, return_inverse=True)\n",
    "    replaceable = np.isin(flat, to_replace)\n",
    "\n",
    "    # Create a replacement array in which there is a 1:1 relation between\n",
    "    # uniques and the replacement values, so that we can use the inverse index\n",
    "    # to select replacement values.\n",
    "    valid = np.isin(to_replace, uniques, assume_unique=True)\n",
    "    # Remove to_replace values that are not present in da. If no overlap\n",
    "    # exists between to_replace and the values in da, just return a copy.\n",
    "    if not valid.any():\n",
    "        return da.copy()\n",
    "    to_replace = to_replace[valid]\n",
    "    value = value[valid]\n",
    "\n",
    "    replacement = np.zeros_like(uniques)\n",
    "    replacement[np.searchsorted(uniques, to_replace)] = value\n",
    "\n",
    "    out = flat.copy()\n",
    "    out[replaceable] = replacement[index[replaceable]]\n",
    "    return da.copy(data=out.reshape(da.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_gridded(gridded_da: xr.DataArray, station_da: xr.DataArray):\n",
    "    gridded_arr = gridded_da.values\n",
    "    gridded_mean = np.nanmean(gridded_arr)\n",
    "    gridded_std = np.nanstd(gridded_arr)\n",
    "\n",
    "    station_mean = station_da.values[0]\n",
    "    station_std = station_mean / 10\n",
    "\n",
    "    # n_w_nan = gridded_arr.flatten()\n",
    "    # n = n_w_nan[~np.isnan(n_w_nan)]\n",
    "    n = np.linspace(station_mean - 4 * station_std, station_mean + 4 * station_std, 100)\n",
    "    # ratio = (\n",
    "    #     norm.pdf(n, loc=gridded_mean, scale=gridded_std)\n",
    "    #     / norm.pdf(n, loc=station_mean, scale=station_std)\n",
    "    # )\n",
    "    # ratio_da = custom_replace(gridded_da, n, ratio)\n",
    "    # print(ratio_da)\n",
    "    ratio_max = (norm.pdf(n, loc=gridded_mean, scale=gridded_std).max()) / (\n",
    "        norm.pdf(n, loc=station_mean, scale=station_std).max()\n",
    "    )\n",
    "    # print(ratio_max)\n",
    "    # print(2*np.log(ratio_da*gridded_std/station_std))\n",
    "\n",
    "    corrected_da = (\n",
    "        station_std\n",
    "        * np.sqrt(\n",
    "            2 * np.log(ratio_max * gridded_std / station_std)\n",
    "            + ((gridded_da - gridded_mean) / gridded_std) ** 2\n",
    "        )\n",
    "        + station_mean\n",
    "    )\n",
    "    return corrected_da\n",
    "\n",
    "\n",
    "corrected_tmin_da = correct_gridded(\n",
    "    gridded_tmin_station_da, station_da=station_tmin_da.sel(time=\"2008-07-01\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corrected_tmin_da - gridded_tmin_station_da).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_station_da.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tmin_da.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    gridded_tmin_station_da.values.flatten(),\n",
    "    corrected_tmin_da.values.flatten(),\n",
    ")\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_da[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_station_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_tmin_station_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gridded_tmin_station_da - gridded_tmin_station_mean) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct CHIRTS with Li et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f_d_n` # gridded\n",
    "\n",
    "`f_d_n_prime` # station with artificial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100)\n",
    "x = np.linspace(-100, 100, 100)\n",
    "plt.plot(x, norm.pdf(x, loc=10, scale=1), \"o-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_precip_da = gridded_ds[\"precip\"]\n",
    "gridded_precip_da.sel(time=\"2008-07-01\", method=\"nearest\")\n",
    "plt.plot(station_lon, station_lat, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_precip_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_precip_da = gridded_ds[\"precip\"]\n",
    "gridded_precip_da.isel(time=182).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_precip_da = gridded_ds[\"precip\"]\n",
    "gridded_precip_da.sel(time=\"2008-07-01\").plot()\n",
    "plt.plot(station_lon, station_lat, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_precip_da.sel(time=\"2008-07-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gridded_precip_da.where(\n",
    "        (gridded_precip_da.lat >= (station_lat.item() - 0.125))\n",
    "        & (gridded_precip_da.lat <= (station_lat.item() + 0.125))\n",
    "        & (gridded_precip_da.lon >= (station_lon.item() - 0.125))\n",
    "        & (gridded_precip_da.lon <= (station_lon.item() + 0.125)),\n",
    "        drop=True,\n",
    "    )\n",
    "    .sel(time=\"2008-07-01\")\n",
    "    .plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gridded_precip_da.where(\n",
    "        (gridded_precip_da.lat >= (station_lat.item() - 0.125))\n",
    "        & (gridded_precip_da.lat <= (station_lat.item() + 0.125))\n",
    "        & (gridded_precip_da.lon >= (station_lon.item() - 0.125))\n",
    "        & (gridded_precip_da.lon <= (station_lon.item() + 0.125)),\n",
    "        drop=True,\n",
    "    )\n",
    "    .sel(time=\"2008-07-01\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try quantile mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_time = xr.cftime_range(\n",
    "    \"1971-01-01\", \"2000-12-31\", freq=\"D\", calendar=\"noleap\"\n",
    ")\n",
    "future_time = xr.cftime_range(\"2001-01-01\", \"2030-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "\n",
    "# get_hist_temp_for_lat = lambda val: 273.15 - (val * np.cos(2 * np.pi * historical_time.dayofyear / 365) + 2 * np.random.random_sample((historical_time.size,)) + 273.15 + .1 * (historical_time - historical_time[0]).days / 365)\n",
    "# get_rand = lambda: np.random.rand() if np.random.rand() > .5 else  -np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitudes = np.arange(23,27,1)\n",
    "# some_data = [get_hist_temp_for_lat(val) for val in latitudes]\n",
    "# data = np.array([some_data, np.array(some_data)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-downscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
